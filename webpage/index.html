<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Multi-Scenarios Humanoid Locomotion Workshop @ICCV2025</title>
  <link rel="stylesheet" href="./css/style.css">
  <link
  rel="stylesheet"
  href="https://unpkg.com/swiper/swiper-bundle.min.css"
  />

</head>
<body>

  <div class="nav">
    <div class="nav-container">
      <img src="./img/iccv-navbar-logo.svg" alt="ICCV Logo" style="margin-top: 0px; border-radius: 0px; position: absolute; left: 1em; max-height: 40px;">
      <a href="#">Home</a>
      <a href="#intro">Introduction</a>
      <a href="#OpenSources">Open Sources</a>
      <a href="#rules">Rules</a>
      <a href="#tracks">Tracks</a>
      <!-- <a href="#call">Call for Papers</a> -->
    </div>
  </div>

  
  <div class="title-container">
    <div class="overlay"></div>
    <div class="content" style="text-align: center; margin: 20px">
      <h1>Humanoid–Scene Interaction Challenge in Human-Robot-Scene Interaction and Collaboration</h1>
      <div class="subtitle">
        <a href="https://iccv.thecvf.com/">ICCV 2025</a> Workshop
      </div>
      <div class="subtitle">Oct 20th (Afternoon), 2025</div>
      <div class="subtitle">Honolulu, Hawai'i</div>
    </div>
  </div>


  <div class="container">

    <div class="section" id="intro">
      <h2>Introduction</h2>
      <p>In the real world, robots must not only navigate autonomously but also perceive and manipulate the state of objects around them, collaborate with humans, and adapt to dynamically changing scene layouts. This human–object–scene interaction capability represents one of the core challenges in current robotic perception and control research.</p>
      <p>To move robots from controlled laboratories into complex indoor and outdoor environments, they need to reliably, precisely, and safely perform tasks such as recognition, grasping, transport, and interaction across diverse object shapes, materials, and arrangements—truly becoming effective partners in human–robot collaborative work.</p>
      <p>As a key component of the <a href="https://human-robot-scene.github.io/">Human–Robot–Scene Interaction and Collaboration Workshop @ICCV2025</a>, this Humanoid–Scene Interaction Challenge aims to accelerate innovation and performance improvements in this vital area. We provide participants with a high‑quality, scene‑level 3D home‑living room dataset in simulation, a real‑robot test platform, and baseline perception &amp; control code—enabling researchers to focus on developing advanced interactive perception and control strategies.</p>
      <p>Our challenge focuses on breakthroughs in the following core capabilities:</p>
      <ul>
        <li><strong>Interactive Perception &amp; Tracking</strong>: Achieve high‑precision detection and temporal tracking of Humanoid–Scene contact relationships and motion states under heavy occlusion and within complex household scenes.</li>
        <li><strong>Scene‑Level Complex Interaction</strong>: Leverage scene semantics and privileged simulation information to execute multi‑task sequences, ensuring robot actions are natural, fluid, and seamlessly chained.</li>
        <li><strong>Complex Scene Adaptation</strong>: Perform real‑time perception of scene layout and dynamic obstacles (e.g., furniture blocking paths), and adjust path planning and interaction strategies online to maintain task continuity with built‑in fault tolerance and obstacle avoidance.</li>
      </ul>
      <p>By providing unified data formats, evaluation protocols, and baseline implementations, this challenge seeks to drive the development of socially aware, collaborative robot systems capable of seamless sim‑to‑real transfer—laying a solid foundation for efficient human‑robot collaboration in future home, service, and manufacturing environments.</p>
    </div>

    <section class="video-carousel swiper">
      <div class="swiper-wrapper">
        <!-- 每个 slide 放一个 <video> 标签，可按需复制 -->
        <div class="swiper-slide">
          <video src="./video/video1.mp4" controls muted loop defaultPlaybackRate="3.0" autoplay></video>
        </div>
        <div class="swiper-slide">
          <video src="./video/video2.mp4" controls muted loop defaultPlaybackRate="3.0" autoplay></video>
        </div>
        <!-- <div class="swiper-slide">
          <video src="./videos/video3.mp4" controls muted loop></video>
        </div> -->
      </div>
  
      <!-- 如果需要分页器 -->
      <div class="swiper-pagination"></div>
      <!-- 如果需要前后翻页按钮 -->
      <div class="swiper-button-prev"></div>
      <div class="swiper-button-next"></div>
    </section>

    <div class="section" id="OpenSources">
      <h2>Open Source</h2>
      <p>
        <!-- Our Open-source Terrain Benchmark is accessible in <a href="https://github.com/shiki-ta/Humanoid-Terrain-Bench">this repository</a>. You are required to use this repository to participate in the challenge using whichever robot you have, by default we provided Unitree H1_2、G1 and Fourier GR1T2 training code in Isaac Gym. Customized robots are also welcome, as long as they can be trained in Isaac Gym, but you may provide the Isaac Gym deploy config. -->
        Our Open-source HSI Benchmark is accessible in <a href="https://github.com/hyangwork01/Humanoid-Scene-Interaction-Bench">this repository</a>. You are required to use this repository to participate in the challenge using whichever robot you have, by default we provided Unitree H1_2、G1 and SMPL Humanoid training code in Isaac Lab. Customized robots are also welcome, as long as they can be trained in Isaac Lab, but you may provide the Isaac Lab deploy config.
      </p>
      <h3>Resources</h3>
      <ul>
        <li><a href="#">Scene Suite</a>: A diverse set of scenes available in Isaac Lab, including single‑object interaction scenes, bedrooms, living rooms, and more.</li>
        <li><a href="https://github.com/NVlabs/ProtoMotions">Training Platform</a>: A foundational training platform for developing and testing locomotion control policies.</li>
        <li><a href="#">Documentation</a>: Comprehensive documentation to guide participants in using the benchmark effectively.</li>
        <li>Evaluation Metrics: Standardized metrics for assessing interaction performance across different scenes.</li>
      </ul>
    </div>

    <div class="section" id="rules">
      <h2>Rules</h2>
      <ol>
        <li><strong>Eligibility:</strong> Participation is open to academic, industry, and independent teams worldwide. Each team may submit only one entry.</li>
        <li><strong>Robot Requirements:</strong> You may use any simulated humanoid robot compatible with Isaac Lab. Default support is provided for Unitree H1_2, G1, and SMPL Humanoid. Custom robots are welcome but must include deployment configs for Isaac Lab and contact the organizer in advance to adapt the code properly.</li>
        <li><strong>Submission:</strong> Teams must submit both their trained locomotion policy and the model inference code according to the provided submission guidelines. All entries must be reproducible.</li>
        <li><strong>Evaluation:</strong> All policies will be tested on a standardized home‑scene dataset, with performance measured by task completion rate and success rate across tasks of varying difficulty. Under equal conditions, faster approaches are preferred</li>
        <li><strong>Fair Play:</strong> Use of cheating techniques, hard-coding the test environment, or exploiting simulator bugs is strictly prohibited.</li>
        <li><strong>Final Decisions:</strong> The organizing committee reserves the right to make final decisions regarding rule interpretation and winner selection.</li>
      </ol>
    </div>


    <div class="section" id="tracks">
      <h2>Tracks</h2>
      <p>We design interaction tasks of varying difficulty to evaluate the robot’s performance. All evaluation scenarios are drawn from the provided scene dataset.</p>

      <ol>
        <li>
          <strong>Robustness Evaluation:</strong> To assess the robot’s robustness, we increase the complexity of the scenes. For example, we add more objects to the existing home scenario to test the robot’s resilience.
          <div class="img-container">
            <img src="./img/single-obj-scene.png" alt="single-obj-scene for Robustness evaluation">
            <div class="caption">Simple scene interaction for Robustness evaluation</div>
          </div> 
          <div class="img-container">
            <img src="./img/multi-obj-scene.png" alt="multi-obj-scene for Robustness evaluation">
            <div class="caption">Multi scene interaction for Robustness evaluation</div>
          </div>
        </li>
        
        <!-- <li>
          <strong>Extreme Evaluation:</strong> To assess the upper limits of locomotion strategies, we conduct extreme terrain evaluations. For example, the height of stairs will be set significantly higher than in typical scenarios. These challenging conditions are designed to test the maximal capability of the trained humanoid policies, revealing potential failure modes and identifying areas for further improvement.
          <div class="img-container">
            <img src="./img/Extreme.png" alt="Stair for Extreme evaluation">
            <div class="caption">Stair terrain for Extreme evaluation</div>
          </div>
        </li> -->

        <li>
          <strong>Generalization Evaluation:</strong> To evaluate the generalization ability of the interaction policy, we combine multiple interaction tasks within the same scene and introduce a more diverse set of unseen objects in the original scenes. We require the robot to adaptively and coherently complete the tasks and interact with these unseen objects, thereby more rigorously measuring its ability to handle unseen or mixed environments.
        </li>
      </ol>
      
      <div class="terrain-grid">
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room1.png" alt="Living room 1">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room2.png" alt="Living room 2">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room3.png" alt="Living room 3">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room4.png" alt="Living room 4">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room9bed.png" alt="Bed room 9">
          </div>
        </div>

        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room5.png" alt="Living room 5">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room6.png" alt="Living room 6">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room7.png" alt="Living room 7">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room8.png" alt="Living room 8">
          </div>
        </div>

      </div>
      
    
    <!-- <div class="section" id="call">
      <h2>Call for Papers</h2>
      <p>
        We invite submissions of <strong>long papers</strong> (up to 8 pages excluding references) and <strong>short papers</strong> (up to 4 pages excluding references) that explore cutting-edge research in multi-terrain humanoid locomotion.
      </p>

      <h3>Paper Topics</h3>
      <p>
        Topics of interest include, but are not limited to:
      </p>
      
      <div class="call-topics">
        <div class="topic-card">
          <h4>Multi-Terrain Locomotion</h4>
          <p>Control strategies for navigating diverse terrains including slopes, stairs, rubble, and uneven surfaces</p>
        </div>
        
        <div class="topic-card">
          <h4>Adaptive Control</h4>
          <p>Real-time adaptation to changing terrain conditions and unexpected disturbances</p>
        </div>
        
        <div class="topic-card">
          <h4>Simulation to Reality</h4>
          <p>Transfer learning approaches for bridging the sim-to-real gap in locomotion</p>
        </div>
        
        <div class="topic-card">
          <h4>Energy Efficiency</h4>
          <p>Optimizing power consumption while maintaining stability on challenging terrains</p>
        </div>
        
        <div class="topic-card">
          <h4>Learning Frameworks</h4>
          <p>Reinforcement learning, imitation learning, and hybrid approaches for locomotion</p>
        </div>
        
        <div class="topic-card">
          <h4>Benchmarking & Metrics</h4>
          <p>Standardized evaluation protocols for multi-terrain locomotion performance</p>
        </div>
      </div>

      <div class="important-dates">
        <h3>Important Dates</h3>
        <div class="date-item">
          <div class="date-label">Paper Submission:</div>
          <div>Waiting to decide</div>
        </div>
        <div class="date-item">
          <div class="date-label">Notification to Authors:</div>
          <div>Waiting to decide</div>
        </div>
        <div class="date-item">
          <div class="date-label">Camera-Ready Deadline:</div>
          <div>Waiting to decide</div>
        </div>
        <div class="date-item">
          <div class="date-label">Workshop Date:</div>
          <div>Waiting to decide</div>
        </div>
      </div>

      <h3>Submission Guidelines</h3>
      <p>All submissions must be in PDF format and follow the official ICCV 2025 template. Papers should be submitted through the <a href="#">CMT submission system</a> by the specified deadline.</p>
      <p>As a dedicated sub-area within the Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025, accepted papers will be presented at the workshop and included in the proceedings.</p>
    </div> -->

    <footer>
      <p>© Multi-Scenarios Humanoid Locomotion Challenge in Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025</p>
    </footer>
  </div>
  <script src="https://unpkg.com/swiper/swiper-bundle.min.js"></script>
  <script>
    const videoSwiper = new Swiper('.video-carousel', {
      loop: false,
      rewind: true, 
      slidesPerView: 2,            // 同时展示三张幻灯 :contentReference[oaicite:1]{index=1}
      centeredSlides: true,        // 激活幻灯置于中间 :contentReference[oaicite:2]{index=2}
      spaceBetween: 5,            // 幻灯间距 :contentReference[oaicite:3]{index=3}
      effect: 'coverflow',         // 启用 Coverflow 透视效果 :contentReference[oaicite:4]{index=4}
      coverflowEffect: {           // 微调透视参数 :contentReference[oaicite:5]{index=5}
        rotate: 0,
        stretch: 0,
        depth: 100,
        modifier: 1,
        slideShadows: false,
        scale: 0.8,          // <— 让非中间的 slide 缩小到 0.5 倍
      },
      slideToClickedSlide: true,   // 点击侧边幻灯切换到该幻灯 :contentReference[oaicite:6]{index=6}
      navigation: {
        nextEl: '.swiper-button-next',
        prevEl: '.swiper-button-prev',
      },                            // 启用左右箭头导航 :contentReference[oaicite:7]{index=7}
      autoplay: {
        delay: 5000,
        disableOnInteraction: false,
      }
    });



  </script>
</body>
</html>