<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Multi-Scenarios Humanoid Locomotion Workshop @ICCV2025</title>
  <link rel="stylesheet" href="./css/style.css">
  <link
  rel="stylesheet"
  href="https://unpkg.com/swiper/swiper-bundle.min.css"
  />

</head>
<body>

  <div class="nav">
    <div class="nav-container">
      <img src="./img/iccv-navbar-logo.svg" alt="ICCV Logo" style="margin-top: 0px; border-radius: 0px; position: absolute; left: 1em; max-height: 40px;">
      <a href="#">Home</a>
      <a href="#intro">Introduction</a>
      <a href="#OpenSources">Open Sources</a>
      <a href="#rules">Rules</a>
      <a href="#tracks">Tracks</a>
      <!-- <a href="#call">Call for Papers</a> -->
    </div>
  </div>

  
  <div class="title-container">
    <div class="overlay"></div>
    <div class="content" style="text-align: center; margin: 20px">
      <h1>Humanoid–Object Interaction Challenge in Human-Robot-Scene Interaction and Collaboration</h1>
      <div class="subtitle">
        <a href="https://iccv.thecvf.com/">ICCV 2025</a> Workshop
      </div>
      <div class="subtitle">Oct 20th (Afternoon), 2025</div>
      <div class="subtitle">Honolulu, Hawai'i</div>
    </div>
  </div>


  <div class="container">

    <div class="section" id="intro">
      <h2>Introduction</h2>
      <p>In the real world, robots must not only navigate autonomously but also perceive, manipulate, and interact with objects around them, collaborate effectively with humans, and adapt to dynamic scene layouts. This human–object interaction ability—grounded in physical contact, motion understanding, and environmental awareness—is central to the next generation of robotic perception and control systems.</p>
      <p>To transition robots from controlled laboratory settings into real-world indoor environments, they must be capable of reliably and safely recognizing, grasping, transporting, and interacting with diverse objects of varying shapes, materials, and semantics—ultimately becoming intelligent and helpful collaborators in daily human life.</p>
      <p>As a key component of the <a href="https://human-robot-scene.github.io/">Human–Robot–Scene Interaction and Collaboration Workshop @ICCV2025</a>, this Humanoid–Object Interaction Challenge aims to drive innovation and benchmark progress in this essential capability. We provide a simulation-ready 3D object dataset, a real-robot testing platform, and open-source perception and control baselines—empowering participants to develop more intelligent and robust interactive humanoid agents.</p>
      <p>Our challenge focuses on breakthroughs in the following core capabilities:</p>
      <ul>
        <li><strong>Interactive Perception &amp; Tracking</strong>: Achieve high-precision perception, contact detection, and motion tracking of humanoid–object interactions under occlusion and clutter in realistic home environments.</li>
        <li><strong>Object-Centric Interaction Reasoning</strong>: Learn to use object semantics and contextual affordances to guide multi-step interaction sequences that are natural, purposeful, and temporally coherent.</li>
        <li><strong>Adaptive Task Execution with Static Objects</strong>: Perceive and interact with static objects (e.g., fixed furniture, wall‑mounted controls, immovable appliances), planning and adjusting manipulation strategies in real time to accomplish interaction tasks robustly and reliably.</li>
      </ul>
      <p>Through unified data formats, standardized evaluation metrics, and open baselines, the challenge promotes the development of generalizable, sim-to-real transferable humanoid agents capable of socially intelligent interaction in future home, service, and collaborative environments.</p>
    </div>

    <section class="video-carousel swiper">
      <div class="swiper-wrapper">
        <!-- 每个 slide 放一个 <video> 标签，可按需复制 -->
        <div class="swiper-slide">
          <video src="./video/video1.mp4" controls muted loop defaultPlaybackRate="3.0" autoplay></video>
        </div>
        <div class="swiper-slide">
          <video src="./video/video1.mp4" controls muted loop defaultPlaybackRate="3.0" autoplay></video>
        </div>
        <div class="swiper-slide">
          <video src="./video/video1.mp4" controls muted loop defaultPlaybackRate="3.0" autoplay></video>
        </div>
        <div class="swiper-slide">
          <video src="./video/video1.mp4" controls muted loop defaultPlaybackRate="3.0" autoplay></video>
        </div>
        <div class="swiper-slide">
          <video src="./video/video1.mp4" controls muted loop defaultPlaybackRate="3.0" autoplay></video>
        </div>
        <div class="swiper-slide">
          <video src="./video/video2.mp4" controls muted loop defaultPlaybackRate="3.0" autoplay></video>
        </div>
        <!-- <div class="swiper-slide">
          <video src="./videos/video3.mp4" controls muted loop></video>
        </div> -->
      </div>
  
      <!-- 如果需要分页器 -->
      <div class="swiper-pagination"></div>
      <!-- 如果需要前后翻页按钮 -->
      <div class="swiper-button-prev"></div>
      <div class="swiper-button-next"></div>
    </section>

    <div class="section" id="OpenSources">
      <h2>Open Source</h2>
      <p>
        <!-- Our Open-source Terrain Benchmark is accessible in <a href="https://github.com/shiki-ta/Humanoid-Terrain-Bench">this repository</a>. You are required to use this repository to participate in the challenge using whichever robot you have, by default we provided Unitree H1_2、G1 and Fourier GR1T2 training code in Isaac Gym. Customized robots are also welcome, as long as they can be trained in Isaac Gym, but you may provide the Isaac Gym deploy config. -->
        Our Open-source HOI Benchmark is accessible in <a href="https://github.com/hyangwork01/Humanoid-Object-Interaction-Bench">this repository</a>. You are required to use this repository to participate in the challenge using whichever robot you have, by default we provided Unitree H1_2、G1 and SMPL Humanoid training code in Isaac Lab. Customized robots are also welcome, as long as they can be trained in Isaac Lab, but you may provide the Isaac Lab deploy config.
      </p>
      <h3>Resources</h3>
      <ul>
        <li><a href="#">Object Datasets</a>: A diverse set of objects available in Isaac Lab, including bed,sofa,chair,box and more.</li>
        <li><a href="https://github.com/NVlabs/ProtoMotions">Training Platform</a>: A foundational training platform for developing and testing Humanoid Object Interaction policies.</li>
        <li><a href="#">Documentation</a>: Comprehensive documentation to guide participants in using the benchmark effectively.</li>
        <li>Evaluation Metrics: Standardized metrics for assessing interaction performance across different tasks.</li>
      </ul>
    </div>

    <div class="section" id="rules">
      <h2>Rules</h2>
      <ol>
        <li><strong>Eligibility:</strong> Participation is open to academic, industry, and independent teams worldwide. Each team may submit only one entry.</li>
        <li><strong>Robot Requirements:</strong> You may use any simulated humanoid robot compatible with Isaac Lab. Default support is provided for Unitree H1_2, G1, and SMPL Humanoid. Custom robots are welcome but must include deployment configs for Isaac Lab and contact the organizer in advance to adapt the code properly.</li>
        <li><strong>Submission:</strong> Teams must submit both their trained locomotion policy and the model inference code according to the provided submission guidelines. All entries must be reproducible.</li>
        <li><strong>Evaluation:</strong> All policies will be tested on a standardized home‑scene dataset, with performance measured by task completion rate and success rate across tasks of varying difficulty. Under equal conditions, faster approaches are preferred</li>
        <li><strong>Fair Play:</strong> Use of cheating techniques, hard-coding the test environment, or exploiting simulator bugs is strictly prohibited.</li>
        <li><strong>Final Decisions:</strong> The organizing committee reserves the right to make final decisions regarding rule interpretation and winner selection.</li>
      </ol>
    </div>


    <div class="section" id="tracks">
      <h2>Tracks</h2>
      <p>We design vary interaction tasks  to evaluate the robot’s performance. All evaluation scenarios are drawn from the provided object dataset.</p>

      <ol>
        <li>
          <strong>Robustness Evaluation:</strong> To assess the robot’s robustness under physical disturbances, we test task execution performance when applying **external force perturbations** of varying magnitude and direction. This evaluation examines whether the robot can maintain stable interaction and complete tasks despite unexpected physical pushes or disturbances.
          <!-- <div class="img-container">
            <img src="./img/single-obj-scene.png" alt="single-obj-scene for Robustness evaluation">
            <div class="caption">Simple scene interaction for Robustness evaluation</div>
          </div> 
          <div class="img-container">
            <img src="./img/multi-obj-scene.png" alt="multi-obj-scene for Robustness evaluation">
            <div class="caption">Multi scene interaction for Robustness evaluation</div>
          </div> -->
        </li>
        
        <!-- <li>
          <strong>Extreme Evaluation:</strong> To assess the upper limits of locomotion strategies, we conduct extreme terrain evaluations. For example, the height of stairs will be set significantly higher than in typical scenarios. These challenging conditions are designed to test the maximal capability of the trained humanoid policies, revealing potential failure modes and identifying areas for further improvement.
          <div class="img-container">
            <img src="./img/Extreme.png" alt="Stair for Extreme evaluation">
            <div class="caption">Stair terrain for Extreme evaluation</div>
          </div>
        </li> -->

        <li>
          <strong>Generalization Evaluation:</strong> To evaluate the robot’s ability to generalize across object instances, we assess its interaction performance on **previously unseen objects of the same category**. This tests whether the model can adaptively and coherently interact with new object instances within the learned category, without retraining or fine-tuning.
        </li>
      </ol>
    </div>  


    <div class="section" id="tracks">
      <h2>Tasks</h2>
      <p>The Humanoid–Object Interaction Challenge defines the following six interaction tasks, with success criteria for each:</p>
    
      <ol>
        <li>
          <strong>Lie:</strong>
          The pelvis, ankles, and head must remain on the bed surface for at least 0.3 s, fully within the bed area in a bird’s‑eye view; the head height must be between [H, H+0.4 m], where H is the bed height.
          <div class="img-container">
            <img src="./img/lie_down21_bed_01.png" alt="Lie on a bed">
            <div class="caption">Lie on a bed</div>
          </div>
        </li>
        <li>
          <strong>Push:</strong>
          The box’s center of mass must be within 0.1 m of the target point and hold that position for at least 0.5 s.
          <div class="img-container">
            <img src="./img/walk_with_box_cabinet_01.png" alt="Push a Box">
            <div class="caption">Push a box</div>
          </div>
        </li>
        <li>
          <strong>Touch:</strong>
          The end‑effector (hand) must reach within 1 cm of the target point and make contact within 1 s.
          <div class="img-container">
            <img src="./img/xxx.png" alt="Touch a point">
            <div class="caption">Touch a point</div>
          </div>
        </li>
        <li>
          <strong>Lift:</strong>
          The box must be lifted by at least 0.2 m, and in the final frame both wrist joints must remain within 0.1 m of the box surface.
          <div class="img-container">
            <img src="./img/walk_to_pick_up_box_box_04.png" alt="Lift a box">
            <div class="caption">Lift a box</div>
          </div>
        </li>
        <li>
          <strong>Sit:</strong>
          The pelvis joint must lie within the seating area in a bird’s‑eye view, at a height between [H, H+0.27 m] (H = seat height), and sustain that pose for at least 0.3 s.
          <div class="img-container">
            <img src="./img/chair_mo005_chair_04.png" alt="Sit on a chair">
            <div class="caption">Sit On a chair</div>
          </div>
        </li>
        <li>
          <strong>Claw:</strong>
          At grasp start, the box must sit at least 0.1 m above the table, with both wrists within 0.1 m of the box surface;
          After placement, the box’s center of mass must be within 0.1 m of the target and maintain that for at least 0.5 s.
          <div class="img-container">
            <img src="./img/xxx.png" alt="Claw a box">
            <div class="caption">Claw a box</div>
          </div>
        </li>
      </ol>
    </div>
    
    
      <!-- <div class="terrain-grid">
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room1.png" alt="Living room 1">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room2.png" alt="Living room 2">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room3.png" alt="Living room 3">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room4.png" alt="Living room 4">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room9bed.png" alt="Bed room 9">
          </div>
        </div>

        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room5.png" alt="Living room 5">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room6.png" alt="Living room 6">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room7.png" alt="Living room 7">
          </div>
        </div>
        <div class="terrain-item">
          <div class="img-container">
            <img src="./img/room8.png" alt="Living room 8">
          </div>
        </div>

      </div> -->
      
    
    <!-- <div class="section" id="call">
      <h2>Call for Papers</h2>
      <p>
        We invite submissions of <strong>long papers</strong> (up to 8 pages excluding references) and <strong>short papers</strong> (up to 4 pages excluding references) that explore cutting-edge research in multi-terrain humanoid locomotion.
      </p>

      <h3>Paper Topics</h3>
      <p>
        Topics of interest include, but are not limited to:
      </p>
      
      <div class="call-topics">
        <div class="topic-card">
          <h4>Multi-Terrain Locomotion</h4>
          <p>Control strategies for navigating diverse terrains including slopes, stairs, rubble, and uneven surfaces</p>
        </div>
        
        <div class="topic-card">
          <h4>Adaptive Control</h4>
          <p>Real-time adaptation to changing terrain conditions and unexpected disturbances</p>
        </div>
        
        <div class="topic-card">
          <h4>Simulation to Reality</h4>
          <p>Transfer learning approaches for bridging the sim-to-real gap in locomotion</p>
        </div>
        
        <div class="topic-card">
          <h4>Energy Efficiency</h4>
          <p>Optimizing power consumption while maintaining stability on challenging terrains</p>
        </div>
        
        <div class="topic-card">
          <h4>Learning Frameworks</h4>
          <p>Reinforcement learning, imitation learning, and hybrid approaches for locomotion</p>
        </div>
        
        <div class="topic-card">
          <h4>Benchmarking & Metrics</h4>
          <p>Standardized evaluation protocols for multi-terrain locomotion performance</p>
        </div>
      </div>

      <div class="important-dates">
        <h3>Important Dates</h3>
        <div class="date-item">
          <div class="date-label">Paper Submission:</div>
          <div>Waiting to decide</div>
        </div>
        <div class="date-item">
          <div class="date-label">Notification to Authors:</div>
          <div>Waiting to decide</div>
        </div>
        <div class="date-item">
          <div class="date-label">Camera-Ready Deadline:</div>
          <div>Waiting to decide</div>
        </div>
        <div class="date-item">
          <div class="date-label">Workshop Date:</div>
          <div>Waiting to decide</div>
        </div>
      </div>

      <h3>Submission Guidelines</h3>
      <p>All submissions must be in PDF format and follow the official ICCV 2025 template. Papers should be submitted through the <a href="#">CMT submission system</a> by the specified deadline.</p>
      <p>As a dedicated sub-area within the Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025, accepted papers will be presented at the workshop and included in the proceedings.</p>
    </div> -->

    <footer>
      <p>© Multi-Scenarios Humanoid Locomotion Challenge in Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025</p>
    </footer>
  </div>

  
  <script src="https://unpkg.com/swiper/swiper-bundle.min.js"></script>
  <script>
    const videoSwiper = new Swiper('.video-carousel', {
      loop: false,
      rewind: true, 
      slidesPerView: 2,            // 同时展示三张幻灯 :contentReference[oaicite:1]{index=1}
      centeredSlides: true,        // 激活幻灯置于中间 :contentReference[oaicite:2]{index=2}
      spaceBetween: 5,            // 幻灯间距 :contentReference[oaicite:3]{index=3}
      effect: 'coverflow',         // 启用 Coverflow 透视效果 :contentReference[oaicite:4]{index=4}
      coverflowEffect: {           // 微调透视参数 :contentReference[oaicite:5]{index=5}
        rotate: 0,
        stretch: 0,
        depth: 100,
        modifier: 1,
        slideShadows: false,
        scale: 0.8,          // <— 让非中间的 slide 缩小到 0.5 倍
      },
      slideToClickedSlide: true,   // 点击侧边幻灯切换到该幻灯 :contentReference[oaicite:6]{index=6}
      navigation: {
        nextEl: '.swiper-button-next',
        prevEl: '.swiper-button-prev',
      },                            // 启用左右箭头导航 :contentReference[oaicite:7]{index=7}
      autoplay: {
        delay: 5000,
        disableOnInteraction: false,
      }
    });



  </script>
</body>
</html>